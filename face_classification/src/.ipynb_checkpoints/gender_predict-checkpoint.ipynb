{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.inference import load_image\n",
    "from utils.preprocessor import preprocess_input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for bounding boxes shape\n",
    "gender_offsets = (30, 60)\n",
    "gender_offsets = (10, 10)\n",
    "emotion_offsets = (20, 40)\n",
    "emotion_offsets = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
    "gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = get_labels('fer2013')\n",
    "gender_labels = get_labels('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../../Dataset/img_align_celeba/000001.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection = load_detection_model(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "gender_classifier = load_model(gender_model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting input model shapes for inference\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "gender_target_size = gender_classifier.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_transparency(im, bg_colour=(255, 255, 255)):\n",
    "\n",
    "#     # Only process if image has transparency \n",
    "#     if im.mode in ('RGBA', 'LA') or (im.mode == 'P' and 'transparency' in im.info):\n",
    "\n",
    "#         # Need to convert to RGBA if LA format due to a bug in PIL \n",
    "#         alpha = im.convert('RGBA').split()[-1]\n",
    "\n",
    "#         # Create a new background image of our matt color.\n",
    "#         # Must be RGBA because paste requires both images have the same format\n",
    "\n",
    "#         bg = Image.new(\"RGBA\", im.size, bg_colour + (255,))\n",
    "#         bg.paste(im, mask=alpha)\n",
    "#         return bg\n",
    "\n",
    "#     else:\n",
    "#         return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "def converter(x):\n",
    "\n",
    "    #x has shape (batch, width, height, channels)\n",
    "    return (0.21 * x[:,:,:1]) + (0.72 * x[:,:,1:2]) + (0.07 * x[:,:,-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=None\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import img_as_ubyte\n",
    "rgb_img = io.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 178)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_image=image.img_to_array(image.load_img(image_path, target_size=None,color_mode='rgb'))\n",
    "# gray_img=rgb2gray(rgb_img)\n",
    "# gray_image = img_as_ubyte(gray_img)\n",
    "gray_image=converter(rgb_image)\n",
    "# print(gray_image.shape)\n",
    "gray_image = np.squeeze(gray_image)\n",
    "gray_image = gray_image.astype('uint8')\n",
    "gray_image.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# def gender_preprocess\n",
    "# def gender_predict\n",
    "\n",
    "gray_image=converter(rgb_img)\n",
    "# print(gray_image.shape)\n",
    "gray_image = np.squeeze(gray_image)\n",
    "gray_image = gray_image.astype('uint8')\n",
    "# print('HERE....')\n",
    "faces = detect_faces(face_detection, gray_image)\n",
    "print(len(faces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def emotion_predict(img):\n",
    "    gray_image=converter(img)\n",
    "    # print(gray_image.shape)\n",
    "    gray_image = np.squeeze(gray_image)\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "    faces = detect_faces(face_detection, gray_image)\n",
    "\n",
    "    if len(faces)==0:\n",
    "        rand_index=random.randint(0,6)\n",
    "        ret_arr=np.zeros((1,7))\n",
    "        ret_arr[0][rand_index]=1.0\n",
    "        return ret_arr\n",
    "    x1, x2, y1, y2 = apply_offsets(faces[0], emotion_offsets)\n",
    "\n",
    "    gray_face = gray_image[y1:y2, x1:x2]\n",
    "    gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "\n",
    "    gray_face = preprocess_input(gray_face, True)\n",
    "    gray_face = np.expand_dims(gray_face, 0)\n",
    "    gray_face = np.expand_dims(gray_face, -1)\n",
    "    return emotion_classifier.predict(gray_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.771,  0.554,  1.265,  0.124, 76.321,  0.045, 20.92 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "emotion_predict(rgb_img)*100\n",
    "# np.zeros((1,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,7)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(image.load_img(image_path, target_size=None,color_mode='rgba\n",
    "# image.img_to_array(test_img)\n",
    "# x = np.expand_dims(rgb_img, axis=0)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanation = explainer.explain_instance(rgb_img, emotion_predict, top_labels=1, hide_color=0, num_samples=1000,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fde6c072208>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANoAAAD8CAYAAAAR6LrwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAR4klEQVR4nO3de4xc5X3G8e9TA0ZAKiABxMUtF5moEKUbsAAJgRJRYkBRDJXSgiripqgLKkiJlD/KRWpRqkioxSBFTR2BsDAVgdA4BKtya4wVJYpUEmzicokxvsQJiy07IRWgEjnY/PrHOdMcj2d2Z+dc5lyej7TamXdmdt6zO8++73nnzO8oIjCzcv3epDtg1gUOmlkFHDSzCjhoZhVw0Mwq4KCZVaC0oEm6RtI2STsk3VnW85g1gcp4H03SAuB14GpgBngBuCkiflr4k5k1QFkj2iXAjojYFRG/BZ4ElpX0XGa1d1RJP/dM4I3M9Rng0mF3PkYL41iOL6kr1mXnf/y9wn7W6y8dN+vt7/I/v4qIUwbdVlbQNKDtsDmqpGlgGuBYjuNSXVVSV6zL1q/fUtjPWnrG1Ky3Pxff/vmw28qaOs4AizLXzwL2ZO8QEQ9FxJKIWHI0C0vqhlk9lBW0F4DFks6RdAxwI7C2pOcyq71Spo4RcVDSHcB6YAGwKiJeLeO5zJqgrH00ImIdsK6sn29Wpbn2z+biI0PMKuCgmVXAQTOrgINmrZZ336ooDpq12vo9+d+wLiKsDppZBRw0swo4aGazKGofz0Gz1ipi/6woDppZBRw0swo4aGYVcNDMKlDa0ftmk1KnRZAej2hmQxR5+JaDZq1Tl+Mbs8YOmqRFkr4naaukVyV9MW2/V9KbkrakX9cV112zZsqzj3YQ+HJEvCjpQ8BmSRvS2x6MiPvzd8+sHcYOWkTsBfaml9+VtJWknqPZRNXliP2sQvbRJJ0NfAL4Udp0h6SXJK2SdFIRz2HWZLmDJukEYA3wpYh4B1gJnAdMkYx4K4Y8blrSJkmb3udA3m6Y1VquoEk6miRkj0fEdwAiYl9EHIqID4CHSerwH8EFVK1L8qw6CngE2BoRD2TaT8/c7QbglfG7ZzY/dXyzGvKtOl4O3Ay8LKm3dXcDN0maIqm1vxu4NVcPzSpWxvtweVYdf8jgk1m4aKpZHx8ZYlYBB82sTxn7eQ6atUodj3MEB82sEg6aWQUcNLMKOGhmfcrYz3PQzCrgoFnrzHdEqmKl0kGzVqpb2Bw0a606vafmoFmrLT1jamDgsm29I0GG3bcIDpp1wqhhK4uDZp0xaMSqKmwOmnXO0jOmWL9ny2FTxmx7GRw066xssLJhK0MRxXl2S3o5LZa6KW07WdIGSdvT766EZbVSxSiWVdSI9qmImIqIJen1O4GNEbEY2JheN6ulJr9hvQxYnV5eDVxf0vOYNUIRQQvgWUmbJU2nbaellYx7FY1PLeB5zBqriPOjXR4ReySdCmyQ9NooD0pDOQ1wLMcV0A2z+so9okXEnvT7fuBpkoKp+3r1HdPv+wc8zgVUrTPyVio+Pj2TDJKOBz5NUjB1LbA8vdty4Jk8z2PWdHmnjqcBTydFizkK+GZE/KekF4CnJN0C/AL4XM7nMWu0XEGLiF3AHw9ofwu4Ks/PNmsTHxliVgEHzawCDppZBRw0swo4aDWX/TiHNZeDVjO9UDlg7VLEIVhWoP6PbdSpwIyNz0GriUGjl0PWHp461oBD1n4OWkWy+16Dvmc5ZO3joFVg/Z4t/7/v1f+9n0PWTg5ayQaNYFXVqbD6cNAqNlvIPJq1l4NWokGB6o1o1i0OWklmmxp636x7HLQSeP/L+o39hrWkjwLfyjSdC/wdcCLw18Av0/a7I2Ld2D1smHFC5tGs/cYOWkRsA6YAJC0A3iQpzvMF4MGIuL+QHjaERzGbTVFTx6uAnRHx84J+nlmrFBW0G4EnMtfvkPSSpFVdqLvv0czmUsRJLo4BPgv8W9q0EjiPZFq5F1gx5HHTkjZJ2vQ+B/J2w6zWihjRrgVejIh9ABGxLyIORcQHwMMkBVWP0JYCqnlHMy+EdEMRQbuJzLSxV6E4dQNJQdVW8pTRRpXr82iSjgOuBm7NNP+jpCmSk1/s7rvNrJPyFlB9D/hwX9vNuXrUEEWMZp42doePDDGrgIM2Bu+b2Xw5aCOa7RPRZnNx0EbkD2taHg7aiBwyy8NBM6uAgzYhXtrvFgdtBJ42Wl4O2hwcMiuCgzYHT/GsCK69PwuPZlYUj2hD9ELmEc2K4KANkB3JyhjVHN7ucdD6eLpoZXDQMhwyK4sXQybAZ/TsnpFGtLSa1X5Jr2TaTpa0QdL29PtJabskfU3SjrQS1kVldb5IkxrNPIp2w6hTx0eBa/ra7gQ2RsRiYGN6HZJiPYvTr2mSqli1NukX+6Sf38o3UtAi4gfAr/ualwGr08urgesz7Y9F4nngxL6CPTbA+j1bHLgWy7OPdlpE7AWIiL2STk3bzwTeyNxvJm3bm+O5OsP7b+1UxqqjBrTFEXeqQQHVuo8gde+fjS5P0Pb1poTp9/1p+wywKHO/s4A9/Q+edAHVppzitu79s9HkCdpaYHl6eTnwTKb98+nq42XA270pZl30n7TdrGyjLu8/AfwX8FFJM5JuAe4Drpa0naSI6n3p3dcBu4AdJCXB/6bwXueUDVkTRowm9NFmN9JiSETcNOSmqwbcN4Db83SqbE0KWY9H32br3CFYTQxZTxP7bInOBa2pIbNma33Q+guf1j1kS8+YmnWKWPf+22CtD1pTFj7mClhWnbfDBmt90OoasvksbNR1EcSHjY1OySLhZP2+To5LdcQCZmHqFrZecPr70x+oufpbdADnWtkc5fdX138KVXguvr05IpYMuq11n0cb9mKoS8h6BoV/1Dolc90+2xvyw46lzO7DDmofld+GGKxVQatbmGaTfTH2j3CzhaHIF/Gg39ew0dbyaU3QmvLC6A/KoMDB4FDNJ3T9v4/5/H6a8rtsklYshrTxhZF3+jiuIn6Xbfx75NWKoFmiKfunXeSgWSkc7sO1Zh/NEpN+gXvFcbDWjGjDFhXqpK79KlJTDnWrWivfsK7rH7nsoHV1u+titjesWzOiZXXlD2vNMWfQhhRP/SdJr6UFUp+WdGLafrak30jakn59o8zOmzXFKCPaoxxZPHUD8LGI+DjwOnBX5radETGVft1WTDfNmm3OoA0qnhoRz0bEwfTq8ySVrmqjrvsq1l1F7KP9FfAfmevnSPqJpO9LuqKAn28j8D+Xesv1Ppqke4CDwONp017gDyLiLUkXA9+VdGFEvDPgsdMktfk5luPydOMweY8+NyvD2COapOXAZ4C/SCtfEREHIuKt9PJmYCdw/qDHl1VAddCBuF6FtEkbK2iSrgH+FvhsRLyXaT9F0oL08rkkZ5TZVURHi9DWwNV91O59Ervu/SzTnFPHtHjqJ4GPSJoB/p5klXEhsEESwPPpCuOVwFckHQQOAbdFRP9ZaCpXt09YW/fMGbQhxVMfGXLfNcCavJ0qWm8k6w9b2eHzvqL1tPLIkFFUGbJB161bOnf0vj+qPzld/mfTuaBVocsvqEH8++jo1NGVmqxqnQxamftnDvDhi0/+fSQ6GTRwIMrm3+/hOhs0L4aUoykBy1OObxydDVoZL4imvMgsUWXZhc4GzSNad8z2t67qddDZoM3XpAqa2vzV8bhKB20EDlFz9J80pC5VuTobtPmGZ7ZTLDmIzTGp0a6zQYPD3+cZFpZh7ZP+D2mHq+N0MauVdR0tUfULb5IjexHbmrf/nToRYZfV+T96E5R5aJ6D1gKTDpj3Uec2bgHVeyW9mSmUel3mtrsk7ZC0TdLSsjpu1jPpfzSjGGVEexT4Z+CxvvYHI+L+bIOkC4AbgQuBM4DnJJ0fEYcK6KsNMYkFG49i8zNWAdVZLAOeTKth/QzYAVySo382prL/yzdhFKmTPMv7d6S191dJOiltOxN4I3OfmbTtCJKmJW2StOl9DuTohg1S9ojjEW1+xl0MWQn8AxDp9xUkFYs14L4D3z+IiIeAhyBZ3h+zH5byCFNvY41oEbEvIg5FxAfAw/xuejgDLMrc9SxgT74umjXfWCOapNMjYm969QagtyK5FvimpAdIFkMWAz/O3UubVVWjmaeL4xu3gOonJU2RTAt3A7cCRMSrkp4CfkpSk/92rziWr4rKXnUNWVOmzIUWUE3v/1Xgq3k6ZeMpq2DroMKzNj8+MqQFyi401Pv5bQ9YmdvnoNlQHsWK46DZUHUPV1P2z8BBa4X+QAx6AQ66z7D6lk0IWN372M9Ba6FRXoRzfeC17po0mkHHP2FtzdO0gPU4aNYYTT5Q2lNHq7WmjmD9PKKZVcBBM0uVuTDkoJmlvI9mnTOJfTOPaNYpk1oAKbN8uINmtdKWVcZ+DppVYpRTJ9UtZEX2x0Gz0g0LUp3O9lK2cQuofitTPHW3pC1p+9mSfpO57Rtldt7qbVB4euGqa7D6D1guqp9jFVCNiD/vXZa0Ang7c/+dEdHMI1WtEP3nKGuS3icail6BzFVAVZKAPwOeKLRX1ihtnwIWsU1599GuAPZFxPZM2zmSfiLp+5KuGPZAF1BtnzaEbLbP6eUx0vnRJJ0N/HtEfKyvfSWwIyJWpNcXAidExFuSLga+C1wYEe/M9vN9frRmaUOg8hq0H7fg9B3Fnx9N0lHAnwIX99oi4gAkw1NEbJa0Ezgf2DTu81i9OGSJ+f4e8kwd/wR4LSJmeg2STpG0IL18LkkB1V05nsNqxCEb31gFVCPiEZLTM/UvglwJfEXSQeAQcFtEjHomGqspByy/cQuoEhF/OaBtDbAmf7fM2sVH79thPHqVw0Ezh6sCDloLOTj144OKWyB7VIZDVk8OWgs0vRhqF3jq2DKjlAe36jloLefg1YOnjmYVcNDMKuCgmVXAQTOrgINmVgEHzawCDppZBRw0swo4aGYVGKWA6iJJ35O0VdKrkr6Ytp8saYOk7en3k9J2SfqapB2SXpJ0UdkbYVZ3o4xoB4EvR8QfAZcBt0u6ALgT2BgRi4GN6XWAa0lqhSwGpoGVhffarGFGKaC6NyJeTC+/C2wFzgSWAavTu60Grk8vLwMei8TzwImSTi+852YNMq99tLS+4yeAHwGnRcReSMIInJre7UzgjczDZtK2/p/lAqrWGSMHTdIJJIV3vjRHQVQNaDuiSmtEPBQRSyJiydEsHLUbloOP3J+ckYIm6WiSkD0eEd9Jm/f1poTp9/1p+wywKPPws4A9xXTXrJlGWXUU8AiwNSIeyNy0FlieXl4OPJNp/3y6+ngZ8HZvimnWVaN88PNy4Gbg5d550IC7gfuApyTdAvwC+Fx62zrgOmAH8B7whUJ7bNZAoxRQ/SGD97sAjjgzRSRnzbg9Z7/MWsVHhnSEF0Imy0Ezq4CDZlYBB82sAg5aB3j/bPIcNLMKOGgt59GsHhw0swo4aC3m0aw+HDSzCjhoLeXRrF4cNLMKKDkGeMKdkH4J/C/wq0n3pQAfwdtRF1Vvwx9GxCmDbqhF0AAkbYqIJZPuR17ejvqo0zZ46mhWAQfNrAJ1CtpDk+5AQbwd9VGbbajNPppZm9VpRDNrrYkHTdI1kraltfrvnPsR9SFpt6SXJW2RtCltG3hOgjqRtErSfkmvZNoady6FIdtxr6Q307/JFknXZW67K92ObZKWVtnXiQZN0gLg6yT1+i8Abkrr+jfJpyJiKrOMPOycBHXyKHBNX1sTz6XwKEduB8CD6d9kKiLWAaSvqxuBC9PH/Ev6+qvEpEe0S4AdEbErIn4LPElSu7/Jhp2ToDYi4gfAr/uaG3cuhSHbMcwy4MmIOBARPyMph3hJaZ3rM+mgjVSnv8YCeFbSZknTaduwcxLUXa5zKdTMHek0d1Vm6j7R7Zh00Eaq019jl0fERSTTq9slXTnpDpWgaX+jlcB5wBSwF1iRtk90OyYdtEbX6Y+IPen3/cDTJFORYeckqLtWnEshIvZFxKGI+AB4mN9NDye6HZMO2gvAYknnSDqGZGd17YT7NBJJx0v6UO8y8GngFYafk6DuWnEuhb79xxtI/iaQbMeNkhZKOodkcefHlXUsIib6RVKn/3VgJ3DPpPszj36fC/x3+vVqr+/Ah0lW7ban30+edF8H9P0JkmnV+yT/6W8Z1m+SKdfX07/Py8CSSfd/ju3417SfL5GE6/TM/e9Jt2MbcG2VffWRIWYVmPTU0awTHDSzCjhoZhVw0Mwq4KCZVcBBM6uAg2ZWAQfNrAL/B4PtNj/1KIEmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "%matplotlib inline\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../../Dataset/img_align_celeba/'\n",
    "output_dict={}\n",
    "for file in os.listdir(data_path):\n",
    "    rgb_img = io.imread(os.path.join(data_path,file))\n",
    "    output=emotion_predict(rgb_img)[0]\n",
    "    output_dict[file]={emotion_labels[i]:np.round(output[i]*100,2) for i in range(len(emotion_labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
